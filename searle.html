<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sam L'Huillier</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Can computers understand things?</name>
              </p>
              <p style="font-size: 17px">
                According to John Searle, a computer cannot understand things because its sole purpose is to follow syntactic rules. He argues that computers - as they are merely manipulating meaningless symbols - cannot possibly understand the semantics behind what they are doing. 
                <br><br>
                In this essay, I will attempt to demonstrate that a computer <i>can</i> understand things. I will do this not by going into advances in modern computing - and why they could resemble human understanding - but why human understanding is actually quite akin to a computer program.	
                <br><br>
                Fundamentally, a digital computer manipulates symbols exclusively in the form of numbers. Every single operation a computer ever does - whether rendering a game or processing a spreadsheet - is arithmetic. In a sense, it‚Äôs just an over-glorified calculator, with some storage. 
                <br><br>
                <!-- And that‚Äôs about it. So it is difficult indeed to make the point that a calculator with storage will ever be able to understand all the things humans can understand. -->
                The main pillar of Searle‚Äôs argument against a computer understanding things is that a computer is a syntactic thing - whereas humans are semantic in nature. A computer, by only ever manipulating symbols without knowing the meaning behind those symbols, can‚Äôt possibly learn or understand anything.	
                <br><br>			
                He develops an interesting thought experiment, known as the Chinese Room Thought Experiment: if an English speaker were in a room, and given a list of mappings between all possible input Chinese phrases and their corresponding outputs Chinese phrases, they could pass off as a fully native Chinese speaker. This is all while understanding absolutely zero Chinese. They are simply manipulating Chinese symbols - much like a computer does. Essentially, the room is like the computer: faking an understanding in Chinese - without actually understanding what the symbols mean.
                <br><br>
                Searle‚Äôs first use of this experiment to discredit the Turing Test: if a computer program were looking up mappings between input Chinese questions and answers to output - it would most certainly not be intelligent - though it might appear intelligent, thus passing the Turing Test. 
                <br><br>
                He then goes further and says that as a result of this thought experiment, any syntactical digital computer cannot understand Chinese. The basis of his argument is that purely because a digital computer manipulates symbols, it cannot understand Chinese. I disagree with this In the coming paragraphs, I will go into why we can consider human understanding - being built on applying meaning to things - is something a computer can very much do too.			
                <br><br>	
                First we need to define what it means to understand. While this could be open to debate, I would say the following definition is reasonable: to understand something one must be able to predict its behaviour in an unseen case. Some sort of intuition must be used in order to devise a prediction about an object‚Äôs behaviour in a case that has not been seen before. So for example, one understands how to add numbers if they can add any two numbers together that they haven‚Äôt seen before.
                <br><br>                
                Understanding is the combination of retrieving relevant information and doing some reasoning on that information. It specifically refers to the process of reasoning about unseen inputs and generalising from seen inputs under some uncertainty. So how does the brain reason about something unknown? How does it create, or essentially fabricate, a prediction about something completely unknown?
                <br><br>                   
                The answer is that it is actually a very well defined process of building upon smaller pieces of knowledge. A human forms an understanding of the world by building chains of very incremental pieces of knowledge on top of each other. It starts with a very limited understanding. For example, the first humans to build weapons will have had to generate very gradual insights about various things; they first picked up that rocks were hard, then that hard things hurt and then that you can for example kick or throw these stones and injure someone. It is interesting to note that these will have been very straightforward observations to pick up and they will have been picked up mostly inadvertently.
                <br><br>                  
                The power of the human brain comes not from being able to perform extraordinarily in calculations or abstract thought, but from being able to draw connections from different places well. And abstract thought comes connecting a series of disjointed pieces of knowledge together. For example, a human cannot ever think of something completely new and abstract that has never been thought of before, they can only recombine the current pieces of knowledge they already have in new ways. This means that the main nuance of human understanding is searching for relevant knowledge and piecing together disjointed pieces of information. Nothing is completely abstract. Thought is the buildup of various combinations of theories. In any situation that requires understanding, humans must search for particular bits of knowledge which may be relevant to that particular situation. Humans, with larger brains than other animals, have a greater capacity for storing and associating more pieces of knowledge and hence can understand more. But, all animals can produce some form of understanding.
                <br><br>                
                Even in highly specialised abstract scenarios like solving maths problems, this theory still applies. Solving a maths problem is more a case of breaking down a problem into sub-problems and solving problems similar to those that have been solved before rather than miraculously coming up with an altogether new abstract solution.		
                <br><br>			
                We can also consider the understanding of language to be to do with forming connections between different places. The meaning of a language comes from knowing which words to use in which situations. It starts out with a very limited number of straightforward meanings. First you learn to say ‚Äúdad‚Äù or ‚Äúmum‚Äù and associate saying those words with getting the attention of your parents. Then you learn to be more specific: you pick up that saying ‚Äúfood‚Äù or ‚Äúwater‚Äù gets you certain physical objects - and you associate those words with particular physical desires - such as hunger and thirst. Each of the fundamental meanings are very simple. Later on, you learn more complex rules: saying ‚Äúplease‚Äù after asking for something could mean that you get whatever you‚Äôre asking for quicker. So you associate saying ‚Äúplease‚Äù if you want something more quickly. It‚Äôs very easy to see how this system can be extrapolated to an entire language. This sequential process of building rules upon rules gives us language, or a human understanding of language.
                <br><br>                         
                                        
                With this in mind, the important part of human understanding is searching for relevant information. In any situation that requires understanding, the brain must be able to search for and filter information that is relevant. This is where meaning comes in. To search for relevant knowledge, the brain must be able to attach some sort of association or meaning to each piece of stored knowledge. It somehow automatically knows what is relevant and what isn‚Äôt. Which means every piece of knowledge inside a brain has meaning. This theory is in a sense how you build an understanding of the world. Each piece of knowledge is associated to another piece of knowledge and you build a sort of chain of knowledge.
                <br><br>                   
                To illustrate this more clearly, imagine if you were playing a game that involved catching small tennis balls, and suddenly you saw a much larger ball flying towards you that you have never seen before. How would you know that you will probably need more force to slow it down and catch it? Well you‚Äôd search for different relevant pieces of information from different contexts. Maybe you can predict that because it is large, it‚Äôll also be heavier as you‚Äôve associated that larger objects tend to be heavier.
                <br><br>                  
                The important thing to note about all of this is that humans do not understand an absolute meaning. They only understand the meaning of things in the context of how they can be used. This definition of meaning is exactly what computers can do too. A number has a meaning to a computer - it can work out whether it is positive or negative and perform operations from there. Then one could also argue that a computer does not understand why it's getting what it‚Äôs getting as input so it can‚Äôt be conscious. But neither do we. We do not understand the fundamental inherent meaning of anything we get as input, only how we can use it. We do not understand why we are on this planet, what the meaning of the universe is nor even how our brains work. We only know how to apply meaning to things that are useful to us.
                <br><br>                
                To conclude, meaning is just a way of associating different pieces of knowledge together and grouping them by usefulness. It is not a higher-order task that computers are incapable of doing. Computers can just as easily apply meaning to inputs in the same way that we can and solely being able to manipulate syntax is not something that prevents a machine from being able to apply meanings and understand things. 
                

            </p>
              
            </td>
            <!-- <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/lhuillier.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/lhuillier2.jpg" class="hoverZoomLink"></a>
            </td> -->
          </tr>
        </tbody></table>
    </body>
    </html>